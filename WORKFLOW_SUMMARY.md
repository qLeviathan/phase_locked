# Latency-Minimized Workflow: Quick Reference

## üéØ **The Problem**: Fragmented Data Flow (805Œºs latency)

```
Current Architecture (INEFFICIENT):

WebSocket ‚Üí Desktop Encoder (15Œºs) ‚îÄ‚Üí Desktop UI
    ‚Üì
    ‚îî‚îÄ‚Üí Mobile Encoder (15Œºs) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí Mobile Agent

Berry Phase: Full N√óN matrix (50Œºs)
Signals: Sequential generation (25Œºs)
GPU: CPU copy + upload (50Œºs)

TOTAL: 805Œºs
ISSUES:
- Duplicate encoding (2√ó)
- Full matrix computation every tick
- Sequential signal generation
- Extra CPU copy to GPU
```

## ‚úÖ **The Solution**: Unified Pipeline (680Œºs latency)

```
Optimized Architecture (EFFICIENT):

WebSocket ‚Üí UNIFIED ENCODER (15Œºs ONCE)
                ‚Üì
            [Zero-Copy Distribution]
                ‚îú‚îÄ‚Üí Desktop Buffer (1Œºs atomic push)
                ‚îú‚îÄ‚Üí Mobile Buffer (1Œºs atomic push)
                ‚îî‚îÄ‚Üí Berry Cache (10Œºs incremental)
                        ‚Üì
                [Parallel Signal Gen] (5Œºs Rayon)
                        ‚Üì
                [Direct GPU] (0Œºs SharedArrayBuffer)
                        ‚Üì
                [Display] (500Œºs GPU shader)

TOTAL: 680Œºs (-125Œºs = 15.5% faster)
BENEFITS:
‚úÖ Single encoding source
‚úÖ Incremental Berry phase
‚úÖ Parallel signal generation
‚úÖ Zero-copy GPU upload
```

---

## üìä Latency Budget Breakdown

### Before Optimization

| Component | Latency | Notes |
|-----------|---------|-------|
| WebSocket parse | 100Œºs | Network stack |
| CORDIC encode (desktop) | 15Œºs | Add/shift only |
| CORDIC encode (mobile) | 15Œºs | **‚ùå DUPLICATE** |
| Berry phase (full N¬≤) | 50Œºs | **‚ùå RECOMPUTES ALL** |
| Signal generation (sequential) | 25Œºs | **‚ùå NOT PARALLEL** |
| IPC transfer | 50Œºs | Zero-copy (OK) |
| GPU copy | 50Œºs | **‚ùå CPU MEMCPY** |
| Display render | 500Œºs | GPU shader (OK) |
| **TOTAL** | **805Œºs** | **‚ùå 195Œºs over target** |

### After Optimization

| Component | Latency | Optimization |
|-----------|---------|--------------|
| WebSocket parse | 100Œºs | (no change) |
| **Unified CORDIC** | **15Œºs** | ‚úÖ **Single source** |
| ~~Duplicate encode~~ | ~~15Œºs~~ | ‚úÖ **Eliminated** |
| **Berry incremental** | **10Œºs** | ‚úÖ **Cache N, not N¬≤** |
| **Parallel signals** | **5Œºs** | ‚úÖ **Rayon vectorized** |
| IPC transfer | 50Œºs | (no change) |
| ~~GPU copy~~ | ~~50Œºs~~ | ‚úÖ **Direct buffer** |
| Display render | 500Œºs | (no change) |
| **TOTAL** | **680Œºs** | ‚úÖ **320Œºs under target** |

---

## üöÄ Implementation Roadmap

### Week 1: Critical Path Optimizations (Highest Impact)

**Day 1-2: Unified Pipeline**
```rust
// File: phi-mamba-desktop/src-tauri/src/unified_pipeline.rs

pub struct UnifiedPipeline {
    encoder: Arc<RwLock<FinancialEncoder>>,  // Single source
    desktop_queue: Arc<ArrayQueue<State>>,
    mobile_queue: Arc<ArrayQueue<State>>,
    berry_cache: Arc<RwLock<BerryCache>>,    // Incremental
}

impl UnifiedPipeline {
    pub fn process(&self, bar: OHLCVBar) {
        let state = self.encoder.write().encode(&bar);  // 15Œºs ONCE

        self.desktop_queue.push(state.clone());         // 1Œºs
        self.mobile_queue.push(state.clone());          // 1Œºs
        self.berry_cache.write().update(&state);        // 10Œºs
    }
}
```

**Impact**: -55Œºs (15Œºs duplicate + 40Œºs Berry phase)

**Day 3-4: Direct GPU Upload**
```typescript
// File: phi-mamba-desktop/src/hooks/useDirectGPU.ts

const sharedBuffer = new SharedArrayBuffer(4096 * 12);
const positions = new Float32Array(sharedBuffer);  // Zero-copy view

geometry.setAttribute('position',
  new THREE.BufferAttribute(positions, 3)  // Direct GPU buffer
);

// Rust writes directly to GPU-visible memory (no CPU copy)
```

**Impact**: -50Œºs (eliminates CPU memcpy)

**Day 5: Integration & Testing**
- Wire unified pipeline into main loop
- Verify latency measurements
- Profile with perf/flamegraph

---

### Week 2: Parallel Processing & Decoupling

**Day 1-2: Parallel Signal Generation**
```rust
// File: phi-mamba-desktop/src-tauri/src/signal_generator.rs

use rayon::prelude::*;

let signals = states.par_iter()
    .filter_map(|state| generate_signal(state))  // 5Œºs parallel
    .collect();
```

**Impact**: -20Œºs (vs 25Œºs sequential)

**Day 3-5: Display Decoupling**
```rust
// Data path: 1000 Hz (all ticks)
tokio::spawn(encode_loop);  // 26Œºs √ó 1000 = 26ms/sec CPU

// Display path: 60 Hz (visual updates only)
setInterval(update_webgl, 16.67ms);  // 0.5ms √ó 60 = 30ms/sec GPU
```

**Impact**: -95ms/sec CPU usage (12.5% ‚Üí 3%)

---

### Week 3: Production Ready

**Real Data Integration**
- WebSocket feeds (Polygon.io, Alpha Vantage)
- Binary protocol parser
- Automatic failover to simulated feed

**Comprehensive Benchmarks**
- Load testing: 1000 ticks/sec sustained
- Latency distribution: P50/P95/P99
- CPU/memory profiling under load

---

## üéì Key Design Principles

### 1. **Encode Once, Distribute Zero-Copy**
```
BAD:  Encode ‚Üí Copy ‚Üí Encode ‚Üí Copy
GOOD: Encode ‚Üí [View] ‚Üí [View] ‚Üí [View]
```

### 2. **Incremental Over Full Recomputation**
```
BAD:  Full N√óN matrix every tick (O(N¬≤))
GOOD: Update N new pairs only (O(N))
```

### 3. **Parallel Where Possible**
```
BAD:  for state in states { compute(state); }
GOOD: states.par_iter().map(compute).collect();
```

### 4. **Direct GPU Access**
```
BAD:  CPU buffer ‚Üí memcpy ‚Üí GPU buffer
GOOD: SharedArrayBuffer (CPU & GPU can both access)
```

### 5. **Decouple Data from Display**
```
BAD:  Update UI every tick (1000 Hz)
GOOD: Data at 1000 Hz, Display at 60 Hz
```

---

## üìà Expected Results

| Metric | Before | After | Improvement |
|--------|--------|-------|-------------|
| **Latency (P50)** | 805Œºs | 680Œºs | **-15.5%** |
| **Latency (P99)** | ~1.2ms | <1ms | **‚úÖ Under budget** |
| **CPU usage** | 12.5% | 3% | **-76%** |
| **Headroom** | -195Œºs | +320Œºs | **‚úÖ 32% margin** |
| **Throughput** | 1000/sec | 1000/sec | (maintained) |

---

## üîß Quick Test

```bash
# Build optimized version
cd phi-mamba-desktop
npm run tauri build

# Run with profiling
RUST_LOG=info npm run tauri dev

# Measure latency in DevTools
performance.measure('latency', 'tick-received', 'frame-rendered');

# Check P99 < 1ms
console.log(performance.getEntriesByName('latency')
  .map(e => e.duration)
  .sort()[Math.floor(length * 0.99)]
);
```

---

## ‚úÖ Success Criteria

**Week 1 Complete When**:
- [ ] Unified pipeline implemented
- [ ] Direct GPU upload working
- [ ] Latency measured < 750Œºs

**Week 2 Complete When**:
- [ ] Parallel signals implemented
- [ ] Display decoupled (60 Hz)
- [ ] CPU usage < 5%

**Week 3 Complete When**:
- [ ] Real data feed integrated
- [ ] P99 latency < 1ms
- [ ] Load tested at 1000 ticks/sec

---

**BOTTOM LINE**:

Single unified pipeline + incremental computation + zero-copy IPC = **680Œºs latency** ‚úÖ

**Next**: Implement `unified_pipeline.rs` (15.5% latency reduction in one file)
